\section{Teilaufgabe C)}
\textbf{Versuchen Sie, die Laufzeit Ihres Programms zu beschleunigen! Dokumentieren Sie
einzelne Verbesserungsideen und die jeweiligen Laufzeitveränderungen für eine lokale
Ausführung Ihres Programms bei der Erzeugung einer 10-tps-Datenbank!}

\subsection{Transaktion}
Um den Durchsatz der Daten zu erhöhen lassen sich die Queries in einer
\textbf{Transaktion}\footnote{\url{https://docs.oracle.com/javase/tutorial/jdbc/basics/transactions.html}} zusammenfassen.  

Dazu muss die Option \textit{autoCommit} abgeschaltet werden und die Transaktion
mit einem manuellen \textit{commit} zum Server geschickt werden. Wir mussten
also unseren \nameref{lst:tpsv1} erweitern. Dafür haben wir 2 neue
Funktionen angelegt.

\begin{lstlisting}
	public void beginTransaktion() {
		try {
			connection.databaseLink.setAutoCommit(false);
		} catch (SQLException e) {
			e.printStackTrace();
		}
	}
	
	public void endAndCommitTransaction() {
		try {
			connection.databaseLink.commit();
			connection.databaseLink.setAutoCommit(true);
		} catch (SQLException e) {
			e.printStackTrace();
		}
	}
\end{lstlisting}


Jetzt brauchen wir noch die funktionalität, um die Funktionen zu benutzen. Damit
wir recht flexibel sind haben wir hierfür ein Konsolen-Menü implementiert.
Dieses erstellt uns dann eine Benchmark Klasse, die der tpsCreator-Klasse alle
nötigen Parameter übergibt und das Benchmarking ausführt.\\

\begin{lstlisting}

	public static int renderMenu() {
		System.out.println("======================- Benchmark Menu -===========================");
		System.out.println("= Wähle bitte eine der folgende Optionen:                         =");
		System.out.println("= (1) Benchmark, Debug Log, incl. drop & create                   =");
		System.out.println("= (2) Benchmark, Debug Log, excl. drop & create                   =");
		System.out.println("= (3) Benchmark, Debug Log, transactions, incl. drop & create     =");
		System.out.println("= (4) Benchmark, Debug Log, transactions, excl. drop & create     =");
		System.out.println("= (5) Benchmark, incl. drop & create                              =");
		System.out.println("= (6) Benchmark, excl. drop & create                              =");
		System.out.println("= (7) Benchmark, transactions, incl. drop & create                =");
		System.out.println("= (8) Benchmark, transactions, excl. drop & create                =");
		System.out.println("===================================================================");
		System.out.print("= Auswahl: ");
		
		return ConsoleReader.readInt();
	}
\end{lstlisting}

 
Die Queries, die mittels einer Transaktion übertragen worden sind, sind bei
einer \textbf{10-tps-Datenbank} um \ca das \textbf{11fache} schneller als die
Queries die einzelnd zum Server geschickt werden.

\begin{figure}[!htbp] 
    \subfigure[mit
    Transactions]{\includegraphics[width=0.49\textwidth]{Bilder/Auswahl_014.png}}
    \subfigure[ohne
    Transaction]{\includegraphics[width=0.49\textwidth]{Bilder/Auswahl_015.png}}
\end{figure} 


\subsection{Prepared Statements}
Selbst in einer Transaktion wird jeder einzelne Query validiert. Wir besitzen
nun aber eine Masse von gleich strukturierten Statements. Jeden einzelnen zu
validieren kostet den Server Zeit und Ressourcen.
 
Wir haben also eine Möglichkeit gesucht eine vorher definierte und validierte
Struktur eines Queries zu bekommen. Dazu hat sich das Konstrukt 
\textbf{Prepared Statement}\footnote{\url{https://docs.oracle.com/javase/tutorial/jdbc/basics/prepared.html}} angeboten.
 
Wie der Name schon sagt, wird der Query im Prepared Stament einmal zum Server
geschickt und validiert. Anschließend kann man mit der Referenz
schnell und sicher die Daten dem Server übergeben.
 
Die umgeschriebene Funktion für Accounts sieht nun folgendermaßen aus:
 \begin{lstlisting}

	public boolean createAccountTupel(int n) {
		int localConst = n*10000;
		int localRandom;		
		try {
			PreparedStatement insertBranches = connection.databaseLink.prepareStatement("INSERT INTO accounts (accid, NAME, balance, address, branchid) VALUES(?, 'account', 0,'test', ?)");
			
			for (int i = 1; i <= localConst; i++) {
				localRandom = ThreadLocalRandom.current().nextInt(1, n + 1);
				insertBranches.setInt(1, i);
				insertBranches.setInt(2, localRandom);
				insertBranches.addBatch();
				
				if(isDebug) {
					System.out.println("INSERT INTO branches (branchid, branchname, balance,address) VALUES(" + i + ", 'branch', 0, 'branch')");
				}
			}

			insertBranches.executeBatch();
			return true;
		} catch (SQLException e) {
			e.printStackTrace();
		}
		return false;
	}
 \end{lstlisting}

Eine gute Eigenschaft von den Prepared Statements ist, dass sie die
Transaktionsverwaltung schon intregriert haben und damit bei gleichen Queries
deutlich schneller sind als ein normales Statement, welches ein und den selben
Query immer und immer wieder ausführt. Diese Eigenschaft besitzt ein
Prepared Statement auch, wenn das Feature \textit{autoCommit} aktiviert ist.

Um aber aussagekräftige Benchmarks zu erhalten, ist gerade das Feature
hinderlich, Wir möchten ja die Queries auch ohne Transaktion ausführen können,
deshalb müssen wir uns eine alte Kopie der Klasse behalten und bei Bedarf
instanziieren.
\begin{center}
\includegraphics[scale=0.8]{Bilder/Auswahl_016.png}
\end{center}
Mit den Prepared Statement ist unser Benchmark jetzt \textbf{36fach} schneller
als unser ursprüngliches Benchmark Tool.

\subsection{IDs vom Server generieren lassen}
Momentan benutzen wir die Ressourcen von unserem Javaprogramm. Da wir aber so
viel wie Möglich vom Server verarbeiten lassen wollen, lagern wir das Generieren
der Branchen-IDs auf unseren Server aus, denn dieser hat mit Sicherheit die
Möglichkeit aus unserem SQL-Query etwas Effizentes zu generieren.

Wir haben uns also den Zufallszahlengenerator von PostgreSQL bedient um uns
eine ID zu genrieren.
\begin{lstlisting}[language=sql]
SELECT rand();
\end{lstlisting}

Diese Funktion liefert uns eine Gleitkommazahl zwischen 0 und 1. Diese Zahl
müssten wir anschließend nur noch $ \cdot  n + 1 $ rechnen um eine zufällige ID
zwischen 1 und $n$ zu erzeugen. Das am Ende stehende $ + 1$ soll dafür sorgen,
dass $n$ auch generiert werden kann. Anschließend haben wir mittels \textit{trunc} noch
die Kommastellen abgeschnitten.

Unser Query, der uns die IDs generiert, sieht dann folgendermaßen aus:
\begin{lstlisting}[language=sql]
SELECT trunc(random() *  n  + 1);
\end{lstlisting}

Diesen Query konnten wir jetzt auf unser Prepared Statement anwenden:
\begin{lstlisting}[language=sql]
INSERT INTO accounts (accid, NAME, balance, address, branchid) VALUES(?, 'account', 0,'test', trunc(random() * n + 1))
\end{lstlisting}

Diese Optimierung  hat uns bei einer \textbf{10-tps-Datenbank} nochmal \ca
500 Millisekunden gespart. Somit sind wir jetzt \textbf{38fach} schneller 
als mit dem Benchmark-Tool, welhes wir als erstes entwickelt hatten.


\subsection{Ändern der Servereinstellungen}

Um ein noch besseres Eregbnis zu erzielen kann man in PostgreSQL die
\textit{asyncronen Commits} aktivieren. Diese sind Stanardmäßig dieaktiviert und
müssen in der Konfigurationsdatei von PostgreSQL ersteinmal aktiviert werden\footnote{https://www.postgresql.org/docs/current/static/wal-async-commit.html}.

Unter Unix-Systemen liegt die Konfigurationsdatei meistens unter \newline
\gqq{\textit{/etc/postgresql/\{versionsnummer\}/main/postgresql.conf}}

Aktivierte \& geänderte Serverkonfiguration:
\begin{lstlisting}
shared_buffers		= 1GB
synchronous_commit	= off
wal_writer_delay	= 5000ms
commit_delay		= 50000
wal_buffers			= 128MB
\end{lstlisting}

Der Performanceschub fällt je nach System unterschiedlich aus. So hat diese
Konfiguration auf den Laptop eines Kommilitonen ein 100 Millisekunden bei einer
\textbf{10-tps-Datenbank} ausgemacht.

\subsection{Adressauflösung von Domainnamen}
Um das Ergebnis noch mehr zu verbessern kam jetzt das Feintuning. Dazu
haebn wir uns jetzt nicht das DBMS zugewendet, sondern haben uns auf die Netzwerkebene begeben.

Eine Datenbank spricht man normalerweiße mit einer Domain an. 
Eine Domain muss aber erst einmal aufgelöst werden.
Das ist ein massiver Nachteil gegenüber der herkömmlicher IP. Jeder
DNS-Server \bzw DNS-Cache muss in irgendeiner Tabelle nachschauen, welche Domain
zu welcher IP gehört. Das kostet beim Verbindungsaufbau wertvolle Zeit.

Selbst der lokale DNS-Cache muss dies \zB für \textbf{localhost} machen.
Möchten wir also die Zugriffszeiten umgehen, brauchen wir einfach nur die IP-Adresse
statt des Domainnamens einzugeben. Dies wäre für \textbf{localhost} entweder \gqq{127.0.0.1} als
IPv4-Schreibweiße oder \gqq{[::1]} als IPv6 Schreibweise.

Wir haben in unser Programm deswegen nur IP-Adressen verwendet um sicher zu gehen, dass wir 
die Zugriffszeiten umgegehen.

-- bis hierher bin ich gekommen .- lg mario
ps: habe nicht auf rächtschreibung geachtet, habe einfach mal drauf los getippz,
vieles ist noch überarbeitungswürdig. Es muss noch das Optimierte Programm in
den Anhang. An einigen Stellen mittels nameref darauf verwiesen werden und
eindeutig sollte der gesamte Text mal durch word geschickt werden.
\clearpage